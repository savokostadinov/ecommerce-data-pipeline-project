{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9c962b-3ba8-4e3e-a89a-b5602e04112b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maski URL:    https://www.fashiongroup.com.mk/obleka/maski/\n",
      "Zenski URL:   https://www.fashiongroup.com.mk/obleka/zenski/\n",
      "Deca URL:     https://www.fashiongroup.com.mk/obleka/devojcinja+momcinja+novorodencinja+bebe-devojcinja+bebe-momcinja\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# --- setup ---\n",
    "driver = webdriver.Chrome()\n",
    "base = \"https://www.fashiongroup.com.mk/\"\n",
    "driver.get(base)\n",
    "\n",
    "# --- collect all “Облека” links ---\n",
    "anchors = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/obleka/']\")\n",
    "\n",
    "markets = {}\n",
    "for a in anchors:\n",
    "    href = a.get_attribute(\"href\")\n",
    "    slug = urlparse(href).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    # normalize both hyphens and pluses into underscores\n",
    "    key  = slug.replace(\"-\", \"_\").replace(\"+\", \"_\")\n",
    "    markets[key] = href\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# now pick them out by the new, fully-normalized keys\n",
    "url_obleka_maski  = markets[\"maski\"]\n",
    "url_obleka_zenski = markets[\"zenski\"]\n",
    "url_obleka_deca   = markets[\"devojcinja_momcinja_novorodencinja_bebe_devojcinja_bebe_momcinja\"]\n",
    "\n",
    "print(\"Maski URL:   \", url_obleka_maski)\n",
    "print(\"Zenski URL:  \", url_obleka_zenski)\n",
    "print(\"Deca URL:    \", url_obleka_deca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25827782-7d21-4edc-80a0-897cdeb26c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_URL             = \"https://www.fashiongroup.com.mk/obleka/maski/\"\n",
    "ITEM_SELECTOR        = \"div.item-data.col-xs-12.col-sm-12\"\n",
    "PAGINATION_SELECTOR  = \"ul.pagination li a\"\n",
    "LISTING_WAIT         = 10\n",
    "REQUESTS_TIMEOUT     = 10\n",
    "HEADERS              = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "# =================\n",
    "\n",
    "def parse_detail(detail_url):\n",
    "    \"\"\"Fetch a detail page via requests+BS4 and extract Brand, Code, Composition.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(detail_url, headers=HEADERS, timeout=REQUESTS_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        return {\"Brand\": \"\", \"Code\": \"\", \"Description\": \"\"}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Brand\n",
    "    brand_el = soup.select_one(\"div.block.product-details-info div.brand\")\n",
    "    brand    = brand_el.get_text(strip=True) if brand_el else \"\"\n",
    "\n",
    "    # Code\n",
    "    code_el = soup.select_one(\"div.block.product-details-info div.code span\")\n",
    "    code    = code_el.get_text(strip=True) if code_el else \"\"\n",
    "\n",
    "    # Composition (“Состав”)\n",
    "    composition = \"\"\n",
    "    for row in soup.select(\"table.product-attrbite-table tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        if len(tds) == 2 and tds[0].get_text(strip=True) == \"Состав\":\n",
    "            composition = tds[1].get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    return {\"Brand\": brand, \"Code\": code, \"Description\": composition}\n",
    "\n",
    "\n",
    "def scrape_all_masks():\n",
    "    # 1) Launch headless Chrome for the listing\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_experimental_option(\"prefs\", {\n",
    "        \"profile.managed_default_content_settings.images\": 2\n",
    "    })\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    wait   = WebDriverWait(driver, LISTING_WAIT)\n",
    "\n",
    "    # 2) Determine how many pages there are\n",
    "    driver.get(BASE_URL)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "    page_links = driver.find_elements(By.CSS_SELECTOR, PAGINATION_SELECTOR)\n",
    "    last_page = 1\n",
    "    for a in page_links:\n",
    "        t = a.text.strip()\n",
    "        if t.isdigit():\n",
    "            last_page = max(last_page, int(t))\n",
    "    print(f\"Detected {last_page} pages.\")\n",
    "\n",
    "    items = []\n",
    "    # 3) Loop through every page\n",
    "    for page in range(1, last_page + 1):\n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        print(f\"→ Scraping listing page {page}/{last_page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, ITEM_SELECTOR)\n",
    "        print(f\"   Found {len(cards)} products.\")\n",
    "\n",
    "        for card in cards:\n",
    "            detail_url = card.find_element(By.CSS_SELECTOR, \"a.product-link\")\\\n",
    "                             .get_attribute(\"href\")\n",
    "            thumb_src  = card.find_element(By.CSS_SELECTOR, \"div.img-wrapper img\")\\\n",
    "                             .get_attribute(\"src\")\n",
    "            image_url  = urljoin(BASE_URL, thumb_src)\n",
    "            name       = card.find_element(By.CSS_SELECTOR, \"div.title a\").text.strip()\n",
    "\n",
    "            # Price splitting\n",
    "            curr = card.find_element(By.CSS_SELECTOR, \"div.current-price .value\").text.strip()\n",
    "            olds = card.find_elements(By.CSS_SELECTOR, \"div.prev-old-price\")\n",
    "            if olds:\n",
    "                regular_price  = olds[0].text.strip()\n",
    "                discount_price = curr\n",
    "            else:\n",
    "                regular_price  = curr\n",
    "                discount_price = \"\"\n",
    "\n",
    "            # Available sizes (no 'disabled')\n",
    "            sizes = [\n",
    "                sz.get_attribute(\"data-productsize-name\") or sz.text.strip()\n",
    "                for sz in card.find_elements(By.CSS_SELECTOR, \"div.product-sizes .item.btn\")\n",
    "                if \"disabled\" not in sz.get_attribute(\"class\")\n",
    "            ]\n",
    "            sizes_str = \", \".join(sizes)\n",
    "\n",
    "            items.append({\n",
    "                \"Image URL\":       image_url,\n",
    "                \"Name\":            name,\n",
    "                \"Regular Price\":   regular_price + \" MKД\",\n",
    "                \"Discount Price\":  (discount_price + \" MKД\") if discount_price else \"\",\n",
    "                \"Available Sizes\": sizes_str,\n",
    "                \"Detail URL\":      detail_url,\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # 4) Enrich each item via requests + BS4\n",
    "    print(\"Fetching detail pages…\")\n",
    "    for idx, it in enumerate(items, 1):\n",
    "        if idx % 20 == 0 or idx == len(items):\n",
    "            print(f\"  → detail {idx}/{len(items)}\")\n",
    "        it.update(parse_detail(it[\"Detail URL\"]))\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = scrape_all_masks()\n",
    "\n",
    "    df = pd.DataFrame(data)[[\n",
    "        \"Image URL\", \"Name\", \"Regular Price\", \"Discount Price\",\n",
    "        \"Available Sizes\", \"Brand\", \"Code\", \"Description\"\n",
    "    ]]\n",
    "    df.to_csv(\n",
    "        \"fashiongroup_maski.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "    print(f\"\\n✅ Scraped {len(df)} products → fashiongroup_maski.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
