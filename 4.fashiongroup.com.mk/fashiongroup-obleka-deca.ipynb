{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511de767-32f2-4680-ad34-c41eacd73917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maski URL:    https://www.fashiongroup.com.mk/obleka/maski/\n",
      "Zenski URL:   https://www.fashiongroup.com.mk/obleka/zenski/\n",
      "Deca URL:     https://www.fashiongroup.com.mk/obleka/devojcinja+momcinja+novorodencinja+bebe-devojcinja+bebe-momcinja\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# --- setup ---\n",
    "driver = webdriver.Chrome()\n",
    "base = \"https://www.fashiongroup.com.mk/\"\n",
    "driver.get(base)\n",
    "\n",
    "# --- collect all “Облека” links ---\n",
    "anchors = driver.find_elements(By.CSS_SELECTOR, \"a[href*='/obleka/']\")\n",
    "\n",
    "markets = {}\n",
    "for a in anchors:\n",
    "    href = a.get_attribute(\"href\")\n",
    "    slug = urlparse(href).path.rstrip(\"/\").split(\"/\")[-1]\n",
    "    # normalize both hyphens and pluses into underscores\n",
    "    key  = slug.replace(\"-\", \"_\").replace(\"+\", \"_\")\n",
    "    markets[key] = href\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# now pick them out by the new, fully-normalized keys\n",
    "url_obleka_maski  = markets[\"maski\"]\n",
    "url_obleka_zenski = markets[\"zenski\"]\n",
    "url_obleka_deca   = markets[\"devojcinja_momcinja_novorodencinja_bebe_devojcinja_bebe_momcinja\"]\n",
    "\n",
    "print(\"Maski URL:   \", url_obleka_maski)\n",
    "print(\"Zenski URL:  \", url_obleka_zenski)\n",
    "print(\"Deca URL:    \", url_obleka_deca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b6173cc-2b07-4b0c-97ff-97503f8e8e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 33 pages.\n",
      "→ Scraping listing page 1/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 2/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 3/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 4/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 5/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 6/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 7/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 8/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 9/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 10/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 11/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 12/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 13/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 14/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 15/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 16/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 17/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 18/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 19/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 20/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 21/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 22/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 23/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 24/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 25/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 26/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 27/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 28/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 29/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 30/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 31/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 32/33\n",
      "   Found 24 products.\n",
      "→ Scraping listing page 33/33\n",
      "   Found 24 products.\n",
      "Fetching detail pages…\n",
      "  → detail 20/792\n",
      "  → detail 40/792\n",
      "  → detail 60/792\n",
      "  → detail 80/792\n",
      "  → detail 100/792\n",
      "  → detail 120/792\n",
      "  → detail 140/792\n",
      "  → detail 160/792\n",
      "  → detail 180/792\n",
      "  → detail 200/792\n",
      "  → detail 220/792\n",
      "  → detail 240/792\n",
      "  → detail 260/792\n",
      "  → detail 280/792\n",
      "  → detail 300/792\n",
      "  → detail 320/792\n",
      "  → detail 340/792\n",
      "  → detail 360/792\n",
      "  → detail 380/792\n",
      "  → detail 400/792\n",
      "  → detail 420/792\n",
      "  → detail 440/792\n",
      "  → detail 460/792\n",
      "  → detail 480/792\n",
      "  → detail 500/792\n",
      "  → detail 520/792\n",
      "  → detail 540/792\n",
      "  → detail 560/792\n",
      "  → detail 580/792\n",
      "  → detail 600/792\n",
      "  → detail 620/792\n",
      "  → detail 640/792\n",
      "  → detail 660/792\n",
      "  → detail 680/792\n",
      "  → detail 700/792\n",
      "  → detail 720/792\n",
      "  → detail 740/792\n",
      "  → detail 760/792\n",
      "  → detail 780/792\n",
      "  → detail 792/792\n",
      "\n",
      "✅ Scraped 792 products → fashiongroup_deca.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_URL             = \"https://www.fashiongroup.com.mk/obleka/devojcinja+momcinja+novorodencinja+bebe-devojcinja+bebe-momcinja\"\n",
    "ITEM_SELECTOR        = \"div.item-data.col-xs-12.col-sm-12\"\n",
    "PAGINATION_SELECTOR  = \"ul.pagination li a\"\n",
    "LISTING_WAIT         = 10\n",
    "REQUESTS_TIMEOUT     = 10\n",
    "HEADERS              = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "# =================\n",
    "\n",
    "def parse_detail(detail_url):\n",
    "    \"\"\"Fetch a detail page via requests+BS4 and extract Brand, Code, Composition.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(detail_url, headers=HEADERS, timeout=REQUESTS_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        return {\"Brand\": \"\", \"Code\": \"\", \"Description\": \"\"}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Brand\n",
    "    brand_el = soup.select_one(\"div.block.product-details-info div.brand\")\n",
    "    brand    = brand_el.get_text(strip=True) if brand_el else \"\"\n",
    "\n",
    "    # Code\n",
    "    code_el = soup.select_one(\"div.block.product-details-info div.code span\")\n",
    "    code    = code_el.get_text(strip=True) if code_el else \"\"\n",
    "\n",
    "    # Composition (“Состав”)\n",
    "    composition = \"\"\n",
    "    for row in soup.select(\"table.product-attrbite-table tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        if len(tds) == 2 and tds[0].get_text(strip=True) == \"Состав\":\n",
    "            composition = tds[1].get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    return {\"Brand\": brand, \"Code\": code, \"Description\": composition}\n",
    "\n",
    "\n",
    "def scrape_all_masks():\n",
    "    # 1) Launch headless Chrome for the listing\n",
    "    opts = webdriver.ChromeOptions()\n",
    "    opts.add_argument(\"--headless=new\")\n",
    "    opts.add_experimental_option(\"prefs\", {\n",
    "        \"profile.managed_default_content_settings.images\": 2\n",
    "    })\n",
    "    driver = webdriver.Chrome(options=opts)\n",
    "    wait   = WebDriverWait(driver, LISTING_WAIT)\n",
    "\n",
    "    # 2) Determine how many pages there are\n",
    "    driver.get(BASE_URL)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "    page_links = driver.find_elements(By.CSS_SELECTOR, PAGINATION_SELECTOR)\n",
    "    last_page = 1\n",
    "    for a in page_links:\n",
    "        t = a.text.strip()\n",
    "        if t.isdigit():\n",
    "            last_page = max(last_page, int(t))\n",
    "    print(f\"Detected {last_page} pages.\")\n",
    "\n",
    "    items = []\n",
    "    # 3) Loop through every page\n",
    "    for page in range(1, last_page + 1):\n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        print(f\"→ Scraping listing page {page}/{last_page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, ITEM_SELECTOR)\n",
    "        print(f\"   Found {len(cards)} products.\")\n",
    "\n",
    "        for card in cards:\n",
    "            detail_url = card.find_element(By.CSS_SELECTOR, \"a.product-link\")\\\n",
    "                             .get_attribute(\"href\")\n",
    "            thumb_src  = card.find_element(By.CSS_SELECTOR, \"div.img-wrapper img\")\\\n",
    "                             .get_attribute(\"src\")\n",
    "            image_url  = urljoin(BASE_URL, thumb_src)\n",
    "            name       = card.find_element(By.CSS_SELECTOR, \"div.title a\").text.strip()\n",
    "\n",
    "            # Price splitting\n",
    "            curr = card.find_element(By.CSS_SELECTOR, \"div.current-price .value\").text.strip()\n",
    "            olds = card.find_elements(By.CSS_SELECTOR, \"div.prev-old-price\")\n",
    "            if olds:\n",
    "                regular_price  = olds[0].text.strip()\n",
    "                discount_price = curr\n",
    "            else:\n",
    "                regular_price  = curr\n",
    "                discount_price = \"\"\n",
    "\n",
    "            # Available sizes (no 'disabled')\n",
    "            sizes = [\n",
    "                sz.get_attribute(\"data-productsize-name\") or sz.text.strip()\n",
    "                for sz in card.find_elements(By.CSS_SELECTOR, \"div.product-sizes .item.btn\")\n",
    "                if \"disabled\" not in sz.get_attribute(\"class\")\n",
    "            ]\n",
    "            sizes_str = \", \".join(sizes)\n",
    "\n",
    "            items.append({\n",
    "                \"Image URL\":       image_url,\n",
    "                \"Name\":            name,\n",
    "                \"Regular Price\":   regular_price + \" MKД\",\n",
    "                \"Discount Price\":  (discount_price + \" MKД\") if discount_price else \"\",\n",
    "                \"Available Sizes\": sizes_str,\n",
    "                \"Detail URL\":      detail_url,\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # 4) Enrich each item via requests + BS4\n",
    "    print(\"Fetching detail pages…\")\n",
    "    for idx, it in enumerate(items, 1):\n",
    "        if idx % 20 == 0 or idx == len(items):\n",
    "            print(f\"  → detail {idx}/{len(items)}\")\n",
    "        it.update(parse_detail(it[\"Detail URL\"]))\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = scrape_all_masks()\n",
    "\n",
    "    df = pd.DataFrame(data)[[\n",
    "        \"Image URL\", \"Name\", \"Regular Price\", \"Discount Price\",\n",
    "        \"Available Sizes\", \"Brand\", \"Code\", \"Description\"\n",
    "    ]]\n",
    "    df.to_csv(\n",
    "        \"fashiongroup_deca.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "    print(f\"\\n✅ Scraped {len(df)} products → fashiongroup_deca.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8c22cf-1133-4edd-af58-f437d68f2d42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 33 pages\n",
      "→ Page 1/33\n",
      "   Found 24 items\n",
      "→ Page 2/33\n",
      "   Found 24 items\n",
      "→ Page 3/33\n",
      "   Found 24 items\n",
      "→ Page 4/33\n",
      "   Found 24 items\n",
      "→ Page 5/33\n",
      "   Found 24 items\n",
      "→ Page 6/33\n",
      "   Found 24 items\n",
      "→ Page 7/33\n",
      "   Found 24 items\n",
      "→ Page 8/33\n",
      "   Found 24 items\n",
      "→ Page 9/33\n",
      "   Found 24 items\n",
      "→ Page 10/33\n",
      "   Found 24 items\n",
      "→ Page 11/33\n",
      "   Found 24 items\n",
      "→ Page 12/33\n",
      "   Found 24 items\n",
      "→ Page 13/33\n",
      "   Found 24 items\n",
      "→ Page 14/33\n",
      "   Found 24 items\n",
      "→ Page 15/33\n",
      "   Found 24 items\n",
      "→ Page 16/33\n",
      "   Found 24 items\n",
      "→ Page 17/33\n",
      "   Found 24 items\n",
      "→ Page 18/33\n",
      "   Found 24 items\n",
      "→ Page 19/33\n",
      "   Found 24 items\n",
      "→ Page 20/33\n",
      "   Found 24 items\n",
      "→ Page 21/33\n",
      "   Found 24 items\n",
      "→ Page 22/33\n",
      "   Found 24 items\n",
      "→ Page 23/33\n",
      "   Found 24 items\n",
      "→ Page 24/33\n",
      "   Found 24 items\n",
      "→ Page 25/33\n",
      "   Found 24 items\n",
      "→ Page 26/33\n",
      "   Found 24 items\n",
      "→ Page 27/33\n",
      "   Found 24 items\n",
      "→ Page 28/33\n",
      "   Found 24 items\n",
      "→ Page 29/33\n",
      "   Found 24 items\n",
      "→ Page 30/33\n",
      "   Found 24 items\n",
      "→ Page 31/33\n",
      "   Found 24 items\n",
      "→ Page 32/33\n",
      "   Found 24 items\n",
      "→ Page 33/33\n",
      "   Found 24 items\n",
      "Fetching details…\n",
      " → detail 20/792\n",
      " → detail 40/792\n",
      " → detail 60/792\n",
      " → detail 80/792\n",
      " → detail 100/792\n",
      " → detail 120/792\n",
      " → detail 140/792\n",
      " → detail 160/792\n",
      " → detail 180/792\n",
      " → detail 200/792\n",
      " → detail 220/792\n",
      " → detail 240/792\n",
      " → detail 260/792\n",
      " → detail 280/792\n",
      " → detail 300/792\n",
      " → detail 320/792\n",
      " → detail 340/792\n",
      " → detail 360/792\n",
      " → detail 380/792\n",
      " → detail 400/792\n",
      " → detail 420/792\n",
      " → detail 440/792\n",
      " → detail 460/792\n",
      " → detail 480/792\n",
      " → detail 500/792\n",
      " → detail 520/792\n",
      " → detail 540/792\n",
      " → detail 560/792\n",
      " → detail 580/792\n",
      " → detail 600/792\n",
      " → detail 620/792\n",
      " → detail 640/792\n",
      " → detail 660/792\n",
      " → detail 680/792\n",
      " → detail 700/792\n",
      " → detail 720/792\n",
      " → detail 740/792\n",
      " → detail 760/792\n",
      " → detail 780/792\n",
      " → detail 792/792\n",
      "\n",
      "✅ Wrote 792 rows to fashiongroup_zenski.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# === CONFIG ===\n",
    "BASE_URL             = \"https://www.fashiongroup.com.mk/obleka/devojcinja+momcinja+novorodencinja+bebe-devojcinja+bebe-momcinja\"\n",
    "ITEM_SELECTOR        = \"div.item-data.col-xs-12.col-sm-12\"\n",
    "PAGINATION_SELECTOR  = \"ul.pagination li a\"\n",
    "LISTING_WAIT         = 10\n",
    "REQUESTS_TIMEOUT     = 10\n",
    "HEADERS              = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "# =================\n",
    "\n",
    "def parse_detail(detail_url):\n",
    "    \"\"\"Fetch a detail page via requests+BS4 and extract Brand, Code, Composition.\"\"\"\n",
    "    try:\n",
    "        resp = requests.get(detail_url, headers=HEADERS, timeout=REQUESTS_TIMEOUT)\n",
    "        resp.raise_for_status()\n",
    "    except requests.RequestException:\n",
    "        return {\"Brand\": \"\", \"Code\": \"\", \"Description\": \"\"}\n",
    "\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    # Brand\n",
    "    brand_el = soup.select_one(\"div.block.product-details-info div.brand\")\n",
    "    brand    = brand_el.get_text(strip=True) if brand_el else \"\"\n",
    "\n",
    "    # Code\n",
    "    code_el = soup.select_one(\"div.block.product-details-info div.code span\")\n",
    "    code    = code_el.get_text(strip=True) if code_el else \"\"\n",
    "\n",
    "    # Composition (“Состав”)\n",
    "    composition = \"\"\n",
    "    for row in soup.select(\"table.product-attrbite-table tr\"):\n",
    "        tds = row.find_all(\"td\")\n",
    "        if len(tds) == 2 and tds[0].get_text(strip=True) == \"Состав\":\n",
    "            composition = tds[1].get_text(strip=True)\n",
    "            break\n",
    "\n",
    "    return {\"Brand\": brand, \"Code\": code, \"Description\": composition}\n",
    "\n",
    "\n",
    "def scrape_all_products():\n",
    "    # 1) Start headless Chrome for the listing pages\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless=new\")\n",
    "    options.add_experimental_option(\"prefs\", {\n",
    "        \"profile.managed_default_content_settings.images\": 2\n",
    "    })\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    wait   = WebDriverWait(driver, LISTING_WAIT)\n",
    "\n",
    "    # 2) Discover last page\n",
    "    driver.get(BASE_URL)\n",
    "    wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "    page_links = driver.find_elements(By.CSS_SELECTOR, PAGINATION_SELECTOR)\n",
    "    last_page = 1\n",
    "    for a in page_links:\n",
    "        text = a.text.strip()\n",
    "        if text.isdigit():\n",
    "            last_page = max(last_page, int(text))\n",
    "    print(f\"Detected {last_page} pages\")\n",
    "\n",
    "    items = []\n",
    "    # 3) Loop through each page\n",
    "    for page in range(1, last_page + 1):\n",
    "        url = f\"{BASE_URL}?page={page}\"\n",
    "        print(f\"→ Page {page}/{last_page}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        wait.until(EC.presence_of_all_elements_located((By.CSS_SELECTOR, ITEM_SELECTOR)))\n",
    "\n",
    "        cards = driver.find_elements(By.CSS_SELECTOR, ITEM_SELECTOR)\n",
    "        print(f\"   Found {len(cards)} items\")\n",
    "\n",
    "        for card in cards:\n",
    "            detail_url = card.find_element(By.CSS_SELECTOR, \"a.product-link\")\\\n",
    "                             .get_attribute(\"href\")\n",
    "            thumb      = card.find_element(By.CSS_SELECTOR, \"div.img-wrapper img\")\\\n",
    "                             .get_attribute(\"src\")\n",
    "            image_url  = urljoin(BASE_URL, thumb)\n",
    "            name       = card.find_element(By.CSS_SELECTOR, \"div.title a\").text.strip()\n",
    "\n",
    "            # Price splitting (fixed logic)\n",
    "            curr = card.find_element(By.CSS_SELECTOR, \"div.current-price .value\").text.strip()\n",
    "            olds = card.find_elements(By.CSS_SELECTOR, \"div.prev-old-price\")\n",
    "            if olds:\n",
    "                # There is a previous price → this is a discounted item\n",
    "                regular_price  = olds[0].text.strip()\n",
    "                discount_price = curr\n",
    "            else:\n",
    "                # No previous price → no discount\n",
    "                regular_price  = curr\n",
    "                discount_price = \"\"\n",
    "\n",
    "            # Add currency suffix\n",
    "            regular_price = f\"{regular_price} MKД\"\n",
    "            if discount_price:\n",
    "                discount_price = f\"{discount_price} MKД\"\n",
    "\n",
    "            # Available sizes\n",
    "            sizes = [\n",
    "                sz.get_attribute(\"data-productsize-name\") or sz.text.strip()\n",
    "                for sz in card.find_elements(By.CSS_SELECTOR, \"div.product-sizes .item.btn\")\n",
    "                if \"disabled\" not in sz.get_attribute(\"class\")\n",
    "            ]\n",
    "            sizes_str = \", \".join(sizes)\n",
    "\n",
    "            items.append({\n",
    "                \"Image URL\":       image_url,\n",
    "                \"Name\":            name,\n",
    "                \"Regular Price\":   regular_price,\n",
    "                \"Discount Price\":  discount_price,\n",
    "                \"Available Sizes\": sizes_str,\n",
    "                \"Detail URL\":      detail_url,\n",
    "            })\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    # 4) Enrich via requests + BS4\n",
    "    print(\"Fetching details…\")\n",
    "    for idx, it in enumerate(items, start=1):\n",
    "        if idx % 20 == 0 or idx == len(items):\n",
    "            print(f\" → detail {idx}/{len(items)}\")\n",
    "        it.update(parse_detail(it[\"Detail URL\"]))\n",
    "\n",
    "    return items\n",
    "\n",
    "\n",
    "def main():\n",
    "    data = scrape_all_products()\n",
    "    df = pd.DataFrame(data)[[\n",
    "        \"Image URL\", \"Name\", \"Regular Price\", \"Discount Price\",\n",
    "        \"Available Sizes\", \"Brand\", \"Code\", \"Description\"\n",
    "    ]]\n",
    "    df.to_csv(\n",
    "        \"fashiongroup_zenski.csv\",\n",
    "        index=False,\n",
    "        encoding=\"utf-8\",\n",
    "        quotechar='\"',\n",
    "        quoting=csv.QUOTE_ALL\n",
    "    )\n",
    "    print(f\"\\n✅ Wrote {len(df)} rows to fashiongroup_zenski.csv\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c10aea-5fb2-4234-8173-6a1f6d9f97de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
